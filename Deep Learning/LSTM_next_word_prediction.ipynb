{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM for next word prediction\n"
      ],
      "metadata": {
        "id": "94A33KDWO-zI"
      },
      "id": "94A33KDWO-zI"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "6134200d",
      "metadata": {
        "id": "6134200d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ef3afb16",
      "metadata": {
        "id": "ef3afb16"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = \"\"\"The US set a 19-run target after their super over but the Green Shirts could only manage 13.\n",
        "\n",
        "The teams went to the super over after the home side levelled the score in the last ball of the 20th over.\n",
        "\n",
        "The US earlier began cautiously against the Green Team as Pakistan got dispirited by the home side’s start.Naseem Shah dismissed Steven Taylor on the first ball of his second over before Andries Gous’s edge on the next delivery dropped short of Iftikhar Ahmed in slips and went to the covers for four.\n",
        "\n",
        "Gous smashed Naseem for another boundary and ended the power play at 44 for the loss of one wicket.\n",
        "\n",
        "Pakistan kept searching for their second wicket as Babar Azam brought on Shadab Khan but Gous and Monank Patel comfortably kept the score ticking for their side.\n",
        "\n",
        "Earlier, Pakistan were held to a total of 159-7 as the co-hosts eyed an upset win.\n",
        "\n",
        "The US chased down a target of 195 to beat fellow non-Test nation Canada in their first Group A encounter but were now up against a far superior bowling attack.\n",
        "\n",
        "Nevertheless, the US could be proud of their efforts in the field against Pakistan given they reduced the 2009 T20 World Cup winners to 26-3 after winning the toss.\n",
        "\n",
        "Left-arm spinner Nosthush Kenjige, who opened the bowling, took 3-30 — including two wickets in two balls — from his maximum four overs and left-arm paceman Saurabh Netravalkar a miserly 2-18.\n",
        "\n",
        "Pakistan were in dire straits at 26-3 inside five overs before a partnership of 72 between skipper Babar (44) and Shadab (40).\n",
        "\n",
        "Shadab and Azam Khan fell in successive balls to Kenjige.\n",
        "\n",
        "Pakistan, runners-up to England at the last T20 World Cup in Australia two years ago, were faltering again at 98-5 before tailender Shaheen Shah Afridi’s unbeaten 23 boosted the total.\n",
        "\n",
        "Earlier, Mohammad Rizwan was superbly caught one-handed at slip by Steven Taylor off Netravalkar before Usman Khan holed out off Kenjige to leave Pakistan 14-2.\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "3f0d8c7b",
      "metadata": {
        "id": "3f0d8c7b"
      },
      "outputs": [],
      "source": [
        "#Tokenizing the text\n",
        "tokenized_text = Tokenizer()\n",
        "tokenized_text.fit_on_texts([text])\n",
        "total_words = len(tokenized_text.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "c67d349f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67d349f",
        "outputId": "ebc55bcc-341d-499c-93e8-dcc4341bc2e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'to': 2,\n",
              " 'in': 3,\n",
              " 'of': 4,\n",
              " 'a': 5,\n",
              " 'pakistan': 6,\n",
              " 'and': 7,\n",
              " 'their': 8,\n",
              " 'for': 9,\n",
              " 'at': 10,\n",
              " 'us': 11,\n",
              " 'over': 12,\n",
              " 'before': 13,\n",
              " 'were': 14,\n",
              " 'after': 15,\n",
              " 'but': 16,\n",
              " 'earlier': 17,\n",
              " 'against': 18,\n",
              " 'as': 19,\n",
              " 'on': 20,\n",
              " 'shadab': 21,\n",
              " 'khan': 22,\n",
              " '3': 23,\n",
              " 'kenjige': 24,\n",
              " 'two': 25,\n",
              " 'target': 26,\n",
              " 'super': 27,\n",
              " 'green': 28,\n",
              " 'could': 29,\n",
              " 'went': 30,\n",
              " 'home': 31,\n",
              " 'side': 32,\n",
              " 'score': 33,\n",
              " 'last': 34,\n",
              " 'ball': 35,\n",
              " 'by': 36,\n",
              " 'naseem': 37,\n",
              " 'shah': 38,\n",
              " 'steven': 39,\n",
              " 'taylor': 40,\n",
              " 'first': 41,\n",
              " 'his': 42,\n",
              " 'second': 43,\n",
              " 'four': 44,\n",
              " 'gous': 45,\n",
              " '44': 46,\n",
              " 'one': 47,\n",
              " 'wicket': 48,\n",
              " 'kept': 49,\n",
              " 'babar': 50,\n",
              " 'azam': 51,\n",
              " 'total': 52,\n",
              " 'up': 53,\n",
              " 'bowling': 54,\n",
              " 't20': 55,\n",
              " 'world': 56,\n",
              " 'cup': 57,\n",
              " '26': 58,\n",
              " 'left': 59,\n",
              " 'arm': 60,\n",
              " '—': 61,\n",
              " 'balls': 62,\n",
              " 'overs': 63,\n",
              " 'netravalkar': 64,\n",
              " '2': 65,\n",
              " 'off': 66,\n",
              " 'set': 67,\n",
              " '19': 68,\n",
              " 'run': 69,\n",
              " 'shirts': 70,\n",
              " 'only': 71,\n",
              " 'manage': 72,\n",
              " '13': 73,\n",
              " 'teams': 74,\n",
              " 'levelled': 75,\n",
              " '20th': 76,\n",
              " 'began': 77,\n",
              " 'cautiously': 78,\n",
              " 'team': 79,\n",
              " 'got': 80,\n",
              " 'dispirited': 81,\n",
              " 'side’s': 82,\n",
              " 'start': 83,\n",
              " 'dismissed': 84,\n",
              " 'andries': 85,\n",
              " 'gous’s': 86,\n",
              " 'edge': 87,\n",
              " 'next': 88,\n",
              " 'delivery': 89,\n",
              " 'dropped': 90,\n",
              " 'short': 91,\n",
              " 'iftikhar': 92,\n",
              " 'ahmed': 93,\n",
              " 'slips': 94,\n",
              " 'covers': 95,\n",
              " 'smashed': 96,\n",
              " 'another': 97,\n",
              " 'boundary': 98,\n",
              " 'ended': 99,\n",
              " 'power': 100,\n",
              " 'play': 101,\n",
              " 'loss': 102,\n",
              " 'searching': 103,\n",
              " 'brought': 104,\n",
              " 'monank': 105,\n",
              " 'patel': 106,\n",
              " 'comfortably': 107,\n",
              " 'ticking': 108,\n",
              " 'held': 109,\n",
              " '159': 110,\n",
              " '7': 111,\n",
              " 'co': 112,\n",
              " 'hosts': 113,\n",
              " 'eyed': 114,\n",
              " 'an': 115,\n",
              " 'upset': 116,\n",
              " 'win': 117,\n",
              " 'chased': 118,\n",
              " 'down': 119,\n",
              " '195': 120,\n",
              " 'beat': 121,\n",
              " 'fellow': 122,\n",
              " 'non': 123,\n",
              " 'test': 124,\n",
              " 'nation': 125,\n",
              " 'canada': 126,\n",
              " 'group': 127,\n",
              " 'encounter': 128,\n",
              " 'now': 129,\n",
              " 'far': 130,\n",
              " 'superior': 131,\n",
              " 'attack': 132,\n",
              " 'nevertheless': 133,\n",
              " 'be': 134,\n",
              " 'proud': 135,\n",
              " 'efforts': 136,\n",
              " 'field': 137,\n",
              " 'given': 138,\n",
              " 'they': 139,\n",
              " 'reduced': 140,\n",
              " '2009': 141,\n",
              " 'winners': 142,\n",
              " 'winning': 143,\n",
              " 'toss': 144,\n",
              " 'spinner': 145,\n",
              " 'nosthush': 146,\n",
              " 'who': 147,\n",
              " 'opened': 148,\n",
              " 'took': 149,\n",
              " '30': 150,\n",
              " 'including': 151,\n",
              " 'wickets': 152,\n",
              " 'from': 153,\n",
              " 'maximum': 154,\n",
              " 'paceman': 155,\n",
              " 'saurabh': 156,\n",
              " 'miserly': 157,\n",
              " '18': 158,\n",
              " 'dire': 159,\n",
              " 'straits': 160,\n",
              " 'inside': 161,\n",
              " 'five': 162,\n",
              " 'partnership': 163,\n",
              " '72': 164,\n",
              " 'between': 165,\n",
              " 'skipper': 166,\n",
              " '40': 167,\n",
              " 'fell': 168,\n",
              " 'successive': 169,\n",
              " 'runners': 170,\n",
              " 'england': 171,\n",
              " 'australia': 172,\n",
              " 'years': 173,\n",
              " 'ago': 174,\n",
              " 'faltering': 175,\n",
              " 'again': 176,\n",
              " '98': 177,\n",
              " '5': 178,\n",
              " 'tailender': 179,\n",
              " 'shaheen': 180,\n",
              " 'afridi’s': 181,\n",
              " 'unbeaten': 182,\n",
              " '23': 183,\n",
              " 'boosted': 184,\n",
              " 'mohammad': 185,\n",
              " 'rizwan': 186,\n",
              " 'was': 187,\n",
              " 'superbly': 188,\n",
              " 'caught': 189,\n",
              " 'handed': 190,\n",
              " 'slip': 191,\n",
              " 'usman': 192,\n",
              " 'holed': 193,\n",
              " 'out': 194,\n",
              " 'leave': 195,\n",
              " '14': 196}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "tokenized_text.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "c4c696fe",
      "metadata": {
        "id": "c4c696fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e953143-9a2f-49b7-a1db-40566fcdb4e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 11]\n",
            "[1, 11, 67]\n",
            "[1, 11, 67, 5]\n",
            "[1, 11, 67, 5, 68]\n",
            "[1, 11, 67, 5, 68, 69]\n",
            "[1, 11, 67, 5, 68, 69, 26]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28, 70]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28, 70, 29]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28, 70, 29, 71]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28, 70, 29, 71, 72]\n",
            "[1, 11, 67, 5, 68, 69, 26, 15, 8, 27, 12, 16, 1, 28, 70, 29, 71, 72, 73]\n",
            "[1, 74]\n",
            "[1, 74, 30]\n",
            "[1, 74, 30, 2]\n",
            "[1, 74, 30, 2, 1]\n",
            "[1, 74, 30, 2, 1, 27]\n",
            "[1, 74, 30, 2, 1, 27, 12]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34, 35]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34, 35, 4]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34, 35, 4, 1]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34, 35, 4, 1, 76]\n",
            "[1, 74, 30, 2, 1, 27, 12, 15, 1, 31, 32, 75, 1, 33, 3, 1, 34, 35, 4, 1, 76, 12]\n",
            "[1, 11]\n",
            "[1, 11, 17]\n",
            "[1, 11, 17, 77]\n",
            "[1, 11, 17, 77, 78]\n",
            "[1, 11, 17, 77, 78, 18]\n",
            "[1, 11, 17, 77, 78, 18, 1]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30, 2]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30, 2, 1]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30, 2, 1, 95]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30, 2, 1, 95, 9]\n",
            "[1, 11, 17, 77, 78, 18, 1, 28, 79, 19, 6, 80, 81, 36, 1, 31, 82, 83, 37, 38, 84, 39, 40, 20, 1, 41, 35, 4, 42, 43, 12, 13, 85, 86, 87, 20, 1, 88, 89, 90, 91, 4, 92, 93, 3, 94, 7, 30, 2, 1, 95, 9, 44]\n",
            "[45, 96]\n",
            "[45, 96, 37]\n",
            "[45, 96, 37, 9]\n",
            "[45, 96, 37, 9, 97]\n",
            "[45, 96, 37, 9, 97, 98]\n",
            "[45, 96, 37, 9, 97, 98, 7]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9, 1]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9, 1, 102]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9, 1, 102, 4]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9, 1, 102, 4, 47]\n",
            "[45, 96, 37, 9, 97, 98, 7, 99, 1, 100, 101, 10, 46, 9, 1, 102, 4, 47, 48]\n",
            "[6, 49]\n",
            "[6, 49, 103]\n",
            "[6, 49, 103, 9]\n",
            "[6, 49, 103, 9, 8]\n",
            "[6, 49, 103, 9, 8, 43]\n",
            "[6, 49, 103, 9, 8, 43, 48]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1, 33]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1, 33, 108]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1, 33, 108, 9]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1, 33, 108, 9, 8]\n",
            "[6, 49, 103, 9, 8, 43, 48, 19, 50, 51, 104, 20, 21, 22, 16, 45, 7, 105, 106, 107, 49, 1, 33, 108, 9, 8, 32]\n",
            "[17, 6]\n",
            "[17, 6, 14]\n",
            "[17, 6, 14, 109]\n",
            "[17, 6, 14, 109, 2]\n",
            "[17, 6, 14, 109, 2, 5]\n",
            "[17, 6, 14, 109, 2, 5, 52]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112, 113]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112, 113, 114]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112, 113, 114, 115]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112, 113, 114, 115, 116]\n",
            "[17, 6, 14, 109, 2, 5, 52, 4, 110, 111, 19, 1, 112, 113, 114, 115, 116, 117]\n",
            "[1, 11]\n",
            "[1, 11, 118]\n",
            "[1, 11, 118, 119]\n",
            "[1, 11, 118, 119, 5]\n",
            "[1, 11, 118, 119, 5, 26]\n",
            "[1, 11, 118, 119, 5, 26, 4]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18, 5]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18, 5, 130]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18, 5, 130, 131]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18, 5, 130, 131, 54]\n",
            "[1, 11, 118, 119, 5, 26, 4, 120, 2, 121, 122, 123, 124, 125, 126, 3, 8, 41, 127, 5, 128, 16, 14, 129, 53, 18, 5, 130, 131, 54, 132]\n",
            "[133, 1]\n",
            "[133, 1, 11]\n",
            "[133, 1, 11, 29]\n",
            "[133, 1, 11, 29, 134]\n",
            "[133, 1, 11, 29, 134, 135]\n",
            "[133, 1, 11, 29, 134, 135, 4]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58, 23]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58, 23, 15]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58, 23, 15, 143]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58, 23, 15, 143, 1]\n",
            "[133, 1, 11, 29, 134, 135, 4, 8, 136, 3, 1, 137, 18, 6, 138, 139, 140, 1, 141, 55, 56, 57, 142, 2, 58, 23, 15, 143, 1, 144]\n",
            "[59, 60]\n",
            "[59, 60, 145]\n",
            "[59, 60, 145, 146]\n",
            "[59, 60, 145, 146, 24]\n",
            "[59, 60, 145, 146, 24, 147]\n",
            "[59, 60, 145, 146, 24, 147, 148]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156, 64]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156, 64, 5]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156, 64, 5, 157]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156, 64, 5, 157, 65]\n",
            "[59, 60, 145, 146, 24, 147, 148, 1, 54, 149, 23, 150, 61, 151, 25, 152, 3, 25, 62, 61, 153, 42, 154, 44, 63, 7, 59, 60, 155, 156, 64, 5, 157, 65, 158]\n",
            "[6, 14]\n",
            "[6, 14, 3]\n",
            "[6, 14, 3, 159]\n",
            "[6, 14, 3, 159, 160]\n",
            "[6, 14, 3, 159, 160, 10]\n",
            "[6, 14, 3, 159, 160, 10, 58]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166, 50]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166, 50, 46]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166, 50, 46, 7]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166, 50, 46, 7, 21]\n",
            "[6, 14, 3, 159, 160, 10, 58, 23, 161, 162, 63, 13, 5, 163, 4, 164, 165, 166, 50, 46, 7, 21, 167]\n",
            "[21, 7]\n",
            "[21, 7, 51]\n",
            "[21, 7, 51, 22]\n",
            "[21, 7, 51, 22, 168]\n",
            "[21, 7, 51, 22, 168, 3]\n",
            "[21, 7, 51, 22, 168, 3, 169]\n",
            "[21, 7, 51, 22, 168, 3, 169, 62]\n",
            "[21, 7, 51, 22, 168, 3, 169, 62, 2]\n",
            "[21, 7, 51, 22, 168, 3, 169, 62, 2, 24]\n",
            "[6, 170]\n",
            "[6, 170, 53]\n",
            "[6, 170, 53, 2]\n",
            "[6, 170, 53, 2, 171]\n",
            "[6, 170, 53, 2, 171, 10]\n",
            "[6, 170, 53, 2, 171, 10, 1]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181, 182]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181, 182, 183]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181, 182, 183, 184]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181, 182, 183, 184, 1]\n",
            "[6, 170, 53, 2, 171, 10, 1, 34, 55, 56, 57, 3, 172, 25, 173, 174, 14, 175, 176, 10, 177, 178, 13, 179, 180, 38, 181, 182, 183, 184, 1, 52]\n",
            "[17, 185]\n",
            "[17, 185, 186]\n",
            "[17, 185, 186, 187]\n",
            "[17, 185, 186, 187, 188]\n",
            "[17, 185, 186, 187, 188, 189]\n",
            "[17, 185, 186, 187, 188, 189, 47]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24, 2]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24, 2, 195]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24, 2, 195, 6]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24, 2, 195, 6, 196]\n",
            "[17, 185, 186, 187, 188, 189, 47, 190, 10, 191, 36, 39, 40, 66, 64, 13, 192, 22, 193, 194, 66, 24, 2, 195, 6, 196, 65]\n"
          ]
        }
      ],
      "source": [
        "# Text pre-processing to prepare it for next word prediction.\n",
        "# We will split each senttence in such way that First we have the first two word to predict the\n",
        "# third word, then we have the 1s, 2nd and 3rd word to predict the 4th word and so on...\n",
        "\n",
        "training_data = [\n",
        "    token_list[:i+1]\n",
        "    for line in text.split('\\n')\n",
        "    for token_list in [tokenized_text.texts_to_sequences([line])[0]]\n",
        "    for i in range(1, len(token_list))\n",
        "]\n",
        "# Below is how our training data looks like. Our goal is to predict the next word based on each list\n",
        "#print(training_data)\n",
        "for i in training_data:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "Now we have sequences of varying lengths but our model will need feature vecters of same length. So we perform madding so that each and every sequence is equal in length to the largest size sequence.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u3u7fNPCRugc"
      },
      "id": "u3u7fNPCRugc"
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = 0\n",
        "for seq in training_data:\n",
        "    if len(seq) > max_sequence_len:\n",
        "        max_sequence_len = len(seq)\n",
        "\n",
        "# Pad the sequences to the maximum length\n",
        "input_sequences = pad_sequences(training_data, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Convert to a numpy array (if needed)\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "# Output the padded sequences\n",
        "#print(input_sequences)"
      ],
      "metadata": {
        "id": "WAr0S22VnoH0"
      },
      "id": "WAr0S22VnoH0",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "cf398edc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf398edc",
        "outputId": "b562f560-d0ba-48e8-98ce-7b7058831877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        1, 11], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "input_sequences[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now for each and every sequence, the last word is the target since it is to be predicted and all the other words are fearures which will help the model predict the last word\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xZw-CH_ESP_J"
      },
      "id": "xZw-CH_ESP_J"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "ce62fdd7",
      "metadata": {
        "id": "ce62fdd7"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "6385ea38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6385ea38",
        "outputId": "ebf4e60c-fb06-4505-a7fa-0768a9ae6929"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 11,\n",
              "       67], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "X[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "bc113505",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc113505",
        "outputId": "ca2c9e98-ec4f-4355-8ed6-cf5b4dadb33d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "y[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Now finally, the y values need to undergo one hot encoding. I.e., we get a one hot vector for each value of y\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qN_HtPYeToP5"
      },
      "id": "qN_HtPYeToP5"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Reshape y to be a 2D array\n",
        "y = np.array(y).reshape(-1, 1)\n",
        "\n",
        "# Initialize the OneHotEncoder with the specified number of categories\n",
        "encoder = OneHotEncoder(categories=[range(total_words)], sparse=False)\n",
        "\n",
        "# Fit and transform y to get the one-hot encoded matrix\n",
        "y = encoder.fit_transform(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JLpuKZEp8wP",
        "outputId": "6eac2b17-e443-451f-faff-28e9502564ab"
      },
      "id": "9JLpuKZEp8wP",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp9-3HtWqbJ7",
        "outputId": "fbca2517-691a-4ec3-f3e9-e89cad6cc4a7"
      },
      "id": "wp9-3HtWqbJ7",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "28ac495c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ac495c",
        "outputId": "44756c7e-0856-438e-a1bc-6c30a05fe20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 52, 100)           19700     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 197)               29747     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 200047 (781.43 KB)\n",
            "Trainable params: 200047 (781.43 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "\n",
        "# Add multiple LSTM layers with dropout for regularization\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(Dropout(0.2))  # Add dropout to prevent overfitting\n",
        "model.add(LSTM(100))  # Another LSTM layer\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "# Output layer with softmax activation\n",
        "model.add(Dense(total_words, activation='softmax'))"
      ],
      "metadata": {
        "id": "RGUXUStnrbdg"
      },
      "id": "RGUXUStnrbdg",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "912b7746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "912b7746",
        "outputId": "a1383ea3-af38-44c8-cd36-8aaa5e0ba209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "11/11 [==============================] - 4s 96ms/step - loss: 5.2739 - accuracy: 0.0571\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 1s 83ms/step - loss: 5.1712 - accuracy: 0.0661\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 1s 73ms/step - loss: 5.0796 - accuracy: 0.0661\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 5.0469 - accuracy: 0.0661\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 5.0258 - accuracy: 0.0661\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.0289 - accuracy: 0.0661\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 5.0254 - accuracy: 0.0661\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 5.0148 - accuracy: 0.0661\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.9866 - accuracy: 0.0661\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.9940 - accuracy: 0.0661\n",
            "Epoch 11/100\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.9873 - accuracy: 0.0661\n",
            "Epoch 12/100\n",
            "11/11 [==============================] - 1s 44ms/step - loss: 4.9643 - accuracy: 0.0661\n",
            "Epoch 13/100\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.9675 - accuracy: 0.0661\n",
            "Epoch 14/100\n",
            "11/11 [==============================] - 0s 22ms/step - loss: 4.9186 - accuracy: 0.0661\n",
            "Epoch 15/100\n",
            "11/11 [==============================] - 0s 22ms/step - loss: 4.8755 - accuracy: 0.0691\n",
            "Epoch 16/100\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 4.8087 - accuracy: 0.0631\n",
            "Epoch 17/100\n",
            "11/11 [==============================] - 0s 27ms/step - loss: 4.7378 - accuracy: 0.0691\n",
            "Epoch 18/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 4.6788 - accuracy: 0.0691\n",
            "Epoch 19/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.5720 - accuracy: 0.0691\n",
            "Epoch 20/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.4753 - accuracy: 0.0721\n",
            "Epoch 21/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.3731 - accuracy: 0.0781\n",
            "Epoch 22/100\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 4.2726 - accuracy: 0.0901\n",
            "Epoch 23/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 4.1863 - accuracy: 0.0841\n",
            "Epoch 24/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 4.0654 - accuracy: 0.0931\n",
            "Epoch 25/100\n",
            "11/11 [==============================] - 0s 30ms/step - loss: 3.9596 - accuracy: 0.1111\n",
            "Epoch 26/100\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 3.8800 - accuracy: 0.1201\n",
            "Epoch 27/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.7743 - accuracy: 0.1291\n",
            "Epoch 28/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.7291 - accuracy: 0.1171\n",
            "Epoch 29/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.5730 - accuracy: 0.1502\n",
            "Epoch 30/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.4700 - accuracy: 0.1532\n",
            "Epoch 31/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.4101 - accuracy: 0.1742\n",
            "Epoch 32/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 3.3296 - accuracy: 0.1321\n",
            "Epoch 33/100\n",
            "11/11 [==============================] - 0s 25ms/step - loss: 3.2832 - accuracy: 0.1682\n",
            "Epoch 34/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.1925 - accuracy: 0.1682\n",
            "Epoch 35/100\n",
            "11/11 [==============================] - 0s 26ms/step - loss: 3.1047 - accuracy: 0.1862\n",
            "Epoch 36/100\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 3.0967 - accuracy: 0.1772\n",
            "Epoch 37/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 3.0191 - accuracy: 0.2042\n",
            "Epoch 38/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8996 - accuracy: 0.2523\n",
            "Epoch 39/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.8924 - accuracy: 0.2312\n",
            "Epoch 40/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8715 - accuracy: 0.2192\n",
            "Epoch 41/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8309 - accuracy: 0.2102\n",
            "Epoch 42/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.7266 - accuracy: 0.2462\n",
            "Epoch 43/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.7102 - accuracy: 0.2492\n",
            "Epoch 44/100\n",
            "11/11 [==============================] - 0s 20ms/step - loss: 2.6360 - accuracy: 0.2853\n",
            "Epoch 45/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6246 - accuracy: 0.2793\n",
            "Epoch 46/100\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 2.5115 - accuracy: 0.2793\n",
            "Epoch 47/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4524 - accuracy: 0.3363\n",
            "Epoch 48/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.4217 - accuracy: 0.3003\n",
            "Epoch 49/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.4052 - accuracy: 0.3183\n",
            "Epoch 50/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3673 - accuracy: 0.3003\n",
            "Epoch 51/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.3261 - accuracy: 0.3333\n",
            "Epoch 52/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 2.2855 - accuracy: 0.3363\n",
            "Epoch 53/100\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 2.2389 - accuracy: 0.3363\n",
            "Epoch 54/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.2137 - accuracy: 0.3574\n",
            "Epoch 55/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.2188 - accuracy: 0.3303\n",
            "Epoch 56/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.1595 - accuracy: 0.3754\n",
            "Epoch 57/100\n",
            "11/11 [==============================] - 0s 19ms/step - loss: 2.1320 - accuracy: 0.3544\n",
            "Epoch 58/100\n",
            "11/11 [==============================] - 0s 26ms/step - loss: 2.1081 - accuracy: 0.3784\n",
            "Epoch 59/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 2.0285 - accuracy: 0.4444\n",
            "Epoch 60/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.0570 - accuracy: 0.3634\n",
            "Epoch 61/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.0415 - accuracy: 0.4024\n",
            "Epoch 62/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9875 - accuracy: 0.3844\n",
            "Epoch 63/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9558 - accuracy: 0.3904\n",
            "Epoch 64/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9249 - accuracy: 0.4384\n",
            "Epoch 65/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.8808 - accuracy: 0.4655\n",
            "Epoch 66/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.8826 - accuracy: 0.4685\n",
            "Epoch 67/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.8267 - accuracy: 0.4685\n",
            "Epoch 68/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8091 - accuracy: 0.4685\n",
            "Epoch 69/100\n",
            "11/11 [==============================] - 0s 23ms/step - loss: 1.8001 - accuracy: 0.4895\n",
            "Epoch 70/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.7631 - accuracy: 0.5015\n",
            "Epoch 71/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.7541 - accuracy: 0.4925\n",
            "Epoch 72/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.6817 - accuracy: 0.5946\n",
            "Epoch 73/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7026 - accuracy: 0.5135\n",
            "Epoch 74/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6734 - accuracy: 0.5345\n",
            "Epoch 75/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6346 - accuracy: 0.5526\n",
            "Epoch 76/100\n",
            "11/11 [==============================] - 0s 21ms/step - loss: 1.6277 - accuracy: 0.5165\n",
            "Epoch 77/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.5906 - accuracy: 0.5766\n",
            "Epoch 78/100\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 1.5871 - accuracy: 0.5646\n",
            "Epoch 79/100\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1.5298 - accuracy: 0.5946\n",
            "Epoch 80/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.5463 - accuracy: 0.5676\n",
            "Epoch 81/100\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1.4910 - accuracy: 0.6066\n",
            "Epoch 82/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.4931 - accuracy: 0.6186\n",
            "Epoch 83/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.4945 - accuracy: 0.5646\n",
            "Epoch 84/100\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 1.5007 - accuracy: 0.5435\n",
            "Epoch 85/100\n",
            "11/11 [==============================] - 0s 15ms/step - loss: 1.4673 - accuracy: 0.6006\n",
            "Epoch 86/100\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1.4453 - accuracy: 0.5946\n",
            "Epoch 87/100\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1.3687 - accuracy: 0.6517\n",
            "Epoch 88/100\n",
            "11/11 [==============================] - 0s 13ms/step - loss: 1.3793 - accuracy: 0.6276\n",
            "Epoch 89/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.3572 - accuracy: 0.6667\n",
            "Epoch 90/100\n",
            "11/11 [==============================] - 0s 35ms/step - loss: 1.3678 - accuracy: 0.6517\n",
            "Epoch 91/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.3034 - accuracy: 0.6907\n",
            "Epoch 92/100\n",
            "11/11 [==============================] - 0s 12ms/step - loss: 1.3107 - accuracy: 0.6667\n",
            "Epoch 93/100\n",
            "11/11 [==============================] - 0s 30ms/step - loss: 1.2797 - accuracy: 0.7057\n",
            "Epoch 94/100\n",
            "11/11 [==============================] - 0s 18ms/step - loss: 1.2651 - accuracy: 0.6787\n",
            "Epoch 95/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.2468 - accuracy: 0.6907\n",
            "Epoch 96/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.2644 - accuracy: 0.6787\n",
            "Epoch 97/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.2455 - accuracy: 0.6937\n",
            "Epoch 98/100\n",
            "11/11 [==============================] - 0s 17ms/step - loss: 1.1997 - accuracy: 0.7207\n",
            "Epoch 99/100\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2085 - accuracy: 0.7087\n",
            "Epoch 100/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.1790 - accuracy: 0.6967\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c0fb419f160>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "c2b46fa7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2b46fa7",
        "outputId": "b81bd2a2-3e25-4553-88e7-2c38013c759f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 608ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Predicting next word based on the input\n",
        "input_text = \"Pakistan kept searching for their second\"\n",
        "token_list = tokenized_text.texts_to_sequences([input_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenized_text.word_index.items()\n",
        "for word, index in tokenized_text.word_index.items():\n",
        "    if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "output_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yMV7QTjVnUF9",
        "outputId": "e029723f-89a5-45a8-a83f-d9d5942f1566"
      },
      "id": "yMV7QTjVnUF9",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wicket'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPeyEtP9wW2y"
      },
      "id": "qPeyEtP9wW2y",
      "execution_count": 73,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}